{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 2 corpora\n",
      "Using cached file: cache/OPUS-bible-uedin-en-fr.txt.zip\n",
      "Using cached file: cache/OPUS-ECDC-en-fr.txt.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1eb7f1e3c9403a9e45ffc5caf27c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/64756 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d0bd62dee44675b8d3fa316ba38020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/63001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2138 samples from 64756 samples. It is now 96.70% of the original dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f0294ca73e4907a92c58426ea40ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/56356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f93a194b1f9406c9f9d470b522b0117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import datasets\n",
    "from lingua import LanguageDetectorBuilder\n",
    "\n",
    "\n",
    "def generate_dataset(source, target, output_dir, cache_dir=None, corpus_list: list[str] | None = None):\n",
    "    \"\"\"Generate a parallel dataset from OPUS for source and target languages.\n",
    "    \n",
    "    Steps:\n",
    "    1. Download Moses-preprocessed parallel corpora from OPUS for the language pair\n",
    "    2. Extract files from downloaded zip archives to working directory\n",
    "    3. Load text files as HuggingFace datasets\n",
    "    4. Concatenate source and target language datasets\n",
    "    5. Add corpus identifier to track data source\n",
    "    \n",
    "    Args:\n",
    "        source: Source language code (e.g. 'en')\n",
    "        target: Target language code (e.g. 'fr') \n",
    "        output_dir: Directory to save final dataset\n",
    "        language_threshold: Minimum confidence threshold for language detection (default: 90)\n",
    "        cache_dir: Optional directory to cache downloaded files (default: None, uses temp dir)\n",
    "        corpus_list: Optional list of corpora to download (default: None, download all)\n",
    "        \n",
    "    Returns:\n",
    "        HuggingFace Dataset containing parallel texts and corpus identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    # download moses dataset\n",
    "    url = f\"https://opus.nlpl.eu/opusapi/?source={source}&target={target}&preprocessing=moses&version=latest\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "   \n",
    "    \n",
    "    if corpus_list:\n",
    "        urls = [corpus['url'] for corpus in response.json()['corpora'] if corpus['corpus'] in corpus_list]\n",
    "    else:\n",
    "        urls = [corpus['url'] for corpus in response.json()['corpora']]\n",
    "    \n",
    "    print(f\"downloading {len(urls)} corpora\")\n",
    "    \n",
    "    # Use cache_dir if provided, otherwise use temp dir\n",
    "    working_dir = cache_dir if cache_dir else tempfile.mkdtemp()\n",
    "    os.makedirs(working_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Download each corpus file if not in cache\n",
    "        downloaded_file_names = defaultdict(dict)\n",
    "        for url in urls:\n",
    "            filename = os.path.basename(url)\n",
    "            corpus_name = url.split('/')[-4]\n",
    "            filepath = os.path.join(working_dir, f\"{corpus_name}-{filename}\")\n",
    "            \n",
    "            if not os.path.exists(filepath):\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded: {filepath}\")\n",
    "            else:\n",
    "                print(f\"Using cached file: {filepath}\")\n",
    "            \n",
    "            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n",
    "                file_list = zip_ref.namelist()                \n",
    "                for file_name in file_list:\n",
    "                    language = file_name.split('.')[-1]\n",
    "                    corpus = file_name.split('.')[0]\n",
    "                    if language == source or language == target:\n",
    "                        extracted_path = os.path.join(working_dir, file_name)\n",
    "                        if not os.path.exists(extracted_path):\n",
    "                            zip_ref.extract(file_name, working_dir)\n",
    "                        downloaded_file_names[corpus][language] = file_name\n",
    "\n",
    "        # load the files as huggingface datasets\n",
    "        all_datasets = []\n",
    "        for corpus, file_names in downloaded_file_names.items():\n",
    "            source_dataset = datasets.load_dataset('text', data_files=os.path.join(working_dir, file_names[source]))['train']\n",
    "            source_dataset = source_dataset.rename_column('text', source)\n",
    "            target_dataset = datasets.load_dataset('text', data_files=os.path.join(working_dir, file_names[target]))['train']\n",
    "            target_dataset = target_dataset.rename_column('text', target)\n",
    "            dataset = datasets.concatenate_datasets([source_dataset, target_dataset], axis=1)\n",
    "            dataset = dataset.map(lambda example: {'corpus': corpus})\n",
    "            all_datasets.append(dataset)\n",
    "        all_datasets: datasets.Dataset = datasets.concatenate_datasets(all_datasets, axis=0)\n",
    "        all_datasets = all_datasets.class_encode_column('corpus')\n",
    "\n",
    "        # Filter out samples that are not in the source or target language\n",
    "        detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "        def check_language(text: str, lang_id: str) -> bool:\n",
    "            language = detector.detect_language_of(text)\n",
    "            if not language:\n",
    "                return False\n",
    "            return language.iso_code_639_1.name.lower() == lang_id\n",
    "        \n",
    "        original_length = len(all_datasets)\n",
    "        all_datasets = all_datasets.filter(lambda example: check_language(example[source], source))\n",
    "        all_datasets = all_datasets.filter(lambda example: check_language(example[target], target))\n",
    "        filtered_length = len(all_datasets)\n",
    "        print(f\"Filtered {original_length - filtered_length} samples from {original_length} samples. It is now {filtered_length/original_length*100:.2f}% of the original dataset.\")\n",
    "        \n",
    "        # do not shuffle to keep sentence order (some are paragraphs)\n",
    "        all_datasets = all_datasets.train_test_split(test_size=0.1, stratify_by_column='corpus', shuffle=False)\n",
    "        all_datasets.save_to_disk(output_dir)\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temp dir if we created one\n",
    "        if not cache_dir and os.path.exists(working_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(working_dir)\n",
    "\n",
    "\n",
    "generate_dataset('en', 'fr', 'output', cache_dir='cache', corpus_list=['bible-uedin', 'ECDC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work on VScode, login from cli, 'hugingface-cli login'\n",
    "if False:\n",
    "    import huggingface_hub\n",
    "    huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39c900fc02b43708ecdf8c02aba5e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2155e077e341d88529bbd1d5eb0c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/57 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94caac3b28540ea97121ab9c6dacfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca279356af1948429248c4d0c1f805bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58471ea145040fab21251d7e519f4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/495 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ShinnosukeU/opus-en-fr-small/commit/f26e9a92e0e68839537f0d700bea318268ff777f', commit_message='Upload dataset', commit_description='', oid='f26e9a92e0e68839537f0d700bea318268ff777f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ShinnosukeU/opus-en-fr-small', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ShinnosukeU/opus-en-fr-small'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_from_disk('output')\n",
    "dataset.push_to_hub('opus-en-fr-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk('output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': Value(dtype='string', id=None),\n",
       " 'fr': Value(dtype='string', id=None),\n",
       " 'corpus': ClassLabel(names=['ECDC', 'bible-uedin'], id=None)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Open thou mine eyes, that I may behold wondrous things out of thy law.',\n",
       " 'fr': 'Ouvre mes yeux, pour que je contemple Les merveilles de ta loi!',\n",
       " 'corpus': 1}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
